---
title: Quickstart
description: Here you'll find information to get started quickly using Tanuki.
---

<Callout intent="success">
Integrate LLM into your workflow in under 5 minutes.
</Callout>

## Quickstart

Welcome to our documentation! Here you'll find information to get started as well as our API Reference.

## Setup

To get started:
1. Install the python or Typescript package
<CodeBlocks>
  <CodeBlock title="Typescript">
  ```bash
  npm install tanuki.ts
  ```
  </CodeBlock>
  <CodeBlock title="Python">
  ```bash
  pip3 install tanuki.py
  ```
  </CodeBlock>
</CodeBlocks>
2. Add the API and authentication keys. To set the API key for the default OpenAI models 

<CodeBlocks>
  <CodeBlock title="Openai setup">
  ```bash
  export OPENAI_API_KEY=sk-...
  ```
  </CodeBlock>
</CodeBlocks>

## Create the function
3. Create a python function stub decorated with `@tanuki.patch` including type hints and a docstring.
4. (Optional) Create another function decorated with `@tanuki.align` containing normal `assert` statements declaring the expected behaviour of your patched function with different inputs. When executing the function, the function annotated with `align` must also be called
5. (Optional) Configure the model you want to use the function for. By default GPT-4 is used but if you want to use any other models supported in our stack, then configure them in the  `@tanuki.patch` operator. You can find out exactly how to configure OpenAI, Amazon Bedrock and Together AI models in the [models](placeholder_url) section.
The patched function can now be called as normal in the rest of your code. 


Here is what the whole script for a a simple classification function would look like:

<CodeBlocks>
  <CodeBlock title="Python">
  ```bash
  import tanuki
  from typing import Optional
  @tanuki.patch
  def classify_sentiment(msg: str) -> Optional[Literal['Good', 'Bad']]:
      """Classifies a message from the user into Good, Bad or None."""

  @tanuki.align
  def align_classify_sentiment():
      assert classify_sentiment("I love you") == 'Good'
      assert classify_sentiment("I hate you") == 'Bad'
      assert not classify_sentiment("People from Phoenix are called Phoenicians")

  align_classify_sentiment()
  print(classify_sentiment("I like you")) # Good
  print(classify_sentiment("Apples might be red")) # None
  ```
  </CodeBlock>
  <CodeBlock title="Typescript">
  ```bash
  class ClassifierSentiment {
    static classifySentiment = patch< "Good" | "Bad", string>()
        `Classifies a message from the user into Good, Bad or null.`;
  }

  Tanuki.align(async (it) => {
     it("Specify how our functions should behave.", async (expect) => {
          await expect(ClassifierSentiment.classifySentiment("I love you")).toEqual('Good');
          await expect(ClassifierSentiment.classifySentiment("I hate you")).toEqual('Bad');
          await expect(ClassifierSentiment.classifySentiment("People from Phoenix are called Phoenicians")).toBeNull();
     })
  })

  console.log(await ClassifierSentiment.classifySentiment("I like you")) // Good
  console.log(await ClassifierSentiment.classifySentiment("Apples might be red")) // Null
  ```
  </CodeBlock>

</CodeBlocks>

## Next steps 

If you want to find out how to create more complex functions and aligns, check out the [Functions](placeholder_url) or the [Aligns](placeholder_url) section.
If you want to find out how to use different teacher and student models, checkout out the [Models](placeholder_url) section.
If you want to find out how Tanuki distills larger models down to smaller models, check out the [Distillation](placeholder_url) section.
If you want to see a whole array of example functions with Tanuki, check out the [Examples](placeholder_url) section.
