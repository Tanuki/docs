---
title: Finetuning
description: Here you'll find information to get started quickly using Tanuki.
---


An advantage of using Tanuki in your workflow is the cost and latency benefits that will be provided as the number of datapoints increases. 

Successful executions of your patched function suitable for finetuning will be persisted to a training dataset, which will be used to distil smaller models for each patched function. Model distillation and pseudo-labelling is a verified way how to cut down on model sizes and gain improvements in latency and memory footprints while incurring insignificant and minor cost to performance ([Distilling Step-by-Step!](https://arxiv.org/pdf/2305.02301.pdf), [ON-POLICY DISTILLATION OF LANGUAGE MODELS](https://arxiv.org/pdf/2306.13649.pdf), [DISTIL-WHISPER](https://arxiv.org/pdf/2311.00430.pdf) etc).

Training smaller function-specific models and deploying them is handled by the Tanuki library, so the user will get the benefits without any additional MLOps or DataOps effort. Currently we support OpenAI GPT style models (GPT-3.5-turbo) and Anyscale models (Llama family and mistral 7B) as finetunable models. See [models](placeholder_url) for which student models are supported. 

![Model distillation workflow](https://github.com/Tanuki/docs/blob/main/fern/docs/assets/distillation_light.png?raw=true)